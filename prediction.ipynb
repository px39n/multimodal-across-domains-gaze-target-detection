{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Library\n",
    "\n",
    "All function need for this script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T19:31:06.220144400Z",
     "start_time": "2023-10-10T19:31:06.213563800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMP is not available\n"
     ]
    }
   ],
   "source": [
    "from deepface import DeepFace\n",
    "import pandas as pd\n",
    "import os\n",
    "from config import get_config,update_config\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "try:\n",
    "    from apex import amp\n",
    "except ImportError:\n",
    "    print(\"AMP is not available\")\n",
    "from datasets.predict_loader import test_loader\n",
    "from datasets.extract_depth_gazefollow import extract_depth\n",
    "from datasets.extract_depth_dino import extract_depth_dino\n",
    "from models import get_model, load_pretrained\n",
    "from utils import get_memory_format\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T20:07:37.318102600Z",
     "start_time": "2023-10-10T20:07:37.313404600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def face_extract(img1_path, img2_path):\n",
    "    result = DeepFace.verify(img1_path = img1_path, img2_path = img2_path, enforce_detection=False)\n",
    "    dict=result[\"facial_areas\"][\"img1\"]\n",
    "    return [dict[\"x\"],dict[\"y\"],dict[\"x\"]+dict[\"w\"],dict[\"y\"]+dict[\"h\"], result[\"distance\"]]\n",
    "\n",
    "def generate_facemask(image_list, start_box, dataset_dir, coefficient=1):\n",
    "    \"\"\"\n",
    "    Generate an annotation file based on the provided image list and reference image.\n",
    "\n",
    "    Parameters:\n",
    "    - image_list: a list of image names or \"ALL\" to indicate all images.\n",
    "    - ref_img: the path to the reference image.\n",
    "    - dataset_dir: the path to the dataset directory.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Detect all images under the specified directory\n",
    "    image_dir = os.path.join(dataset_dir, \"image_original\")\n",
    "    all_images = [os.path.join(image_dir, img_name) for img_name in os.listdir(image_dir) if img_name.endswith(('jpg', 'png', 'jpeg'))]\n",
    "\n",
    "    # Step 2: Update image_list based on the input\n",
    "    if image_list == \"ALL\":\n",
    "        image_list = all_images\n",
    "    else:\n",
    "        image_list = [img for img in all_images if any(s in img for s in image_list)]\n",
    "    image_list.sort(key=lambda x: int(re.search(r'frame_(\\d+).jpg', os.path.basename(x)).group(1)))\n",
    "    rows = []\n",
    "\n",
    "    x0,y0,w0,h0= start_box[0], start_box[1],start_box[2]-start_box[0],start_box[3]-start_box[1]\n",
    "    # Iterating over each image\n",
    "    for img in tqdm(image_list):\n",
    "        # Extract face information\n",
    "        dist=99999999\n",
    "        conf=0.99\n",
    "        face_objs = DeepFace.extract_faces(img_path = img, target_size = (224, 224),detector_backend = 'retinaface')\n",
    "        for face_obj in face_objs:\n",
    "            facebox=face_obj[\"facial_area\"]\n",
    "            x=facebox[\"x\"]\n",
    "            y=facebox[\"y\"]\n",
    "            w=facebox[\"w\"]\n",
    "            h=facebox[\"h\"]\n",
    "            err=(x-x0)**2+(y-y0)**2\n",
    "            if err<dist and abs(x-x0)< w0*coefficient and abs(y-y0)< h0*coefficient:\n",
    "                x1,y1,w1,h1,conf=x,y,w,h,0\n",
    "                dist=err\n",
    "\n",
    "        path = os.path.join(\"image_original\", os.path.basename(img))\n",
    "        if conf==0:\n",
    "            x0,y0,w0,h0=x1,y1,w1,h1\n",
    "        rows.append([path] + [x0,y0,x0+w0,y0+h0]+[conf])\n",
    "\n",
    "    # Convert list of rows to a DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Step 5: Save the dataframe to a file\n",
    "    output_path = os.path.join(dataset_dir, \"head_information.txt\")\n",
    "    df.to_csv(output_path, index=False, header=False)\n",
    "\n",
    "\n",
    "def composition_image(image_path, image, gaze_heatmap_pred, depth, face, head_channel,demo_dir):\n",
    "\n",
    "    # heatmap, original image, depth --> Saving the composite image demo\n",
    "    original_image = Image.open(image_path).convert(\"RGB\")\n",
    "    w,h=original_image.size\n",
    "    composite_img = np.zeros((h*2, w*2, 3), dtype=np.uint8) # Denormalize\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "    #original_image = image.cpu() * std + mean\n",
    "    #original_image = original_image[0][[0, 1, 2], :, :]\n",
    "    #original_image = transforms.ToPILImage()(original_image).convert(\"RGB\")\n",
    "    original_image = np.array(original_image.resize((w, h)))\n",
    "    heatmap_image_resized = gaze_heatmap_pred.resize((w, h))\n",
    "    heatmap_image_resized = np.array(heatmap_image_resized)\n",
    "    heatmap_image_resized = np.stack([heatmap_image_resized * 1, heatmap_image_resized* 1, heatmap_image_resized * 0], axis=2)\n",
    "    masked_image = cv2.addWeighted(original_image, 1, heatmap_image_resized, 2, 0)\n",
    "    composite_img[0:h, 0:w, :] = masked_image\n",
    "\n",
    "    # LEFT DOWN: Depths image\n",
    "    depths_image = transforms.ToPILImage()(depth.cpu()).convert(\"RGB\")\n",
    "    depths_image = np.array(depths_image.resize((w, h)))\n",
    "    composite_img[h:2*h, 0:w, :] = depths_image\n",
    "\n",
    "    # RIGHT UP: Faces image\n",
    "    faces_image = face.cpu()* std + mean\n",
    "    faces_image = faces_image[0][[0, 1, 2], :, :]\n",
    "    faces_image = transforms.ToPILImage()(faces_image).convert(\"RGB\")\n",
    "    faces_image = np.array(faces_image.resize((w, h)))\n",
    "    composite_img[0:h, w:2*w, :] = faces_image\n",
    "\n",
    "    # RIGHT DOWN: Head_channels image (grayscale\n",
    "    heatmap_image = gaze_heatmap_pred.resize((w, h))\n",
    "    heatmap_image = np.array(heatmap_image.resize((w, h)))\n",
    "    #head_channels_image = transforms.ToPILImage()(head_channel.cpu()).convert(\"L\")\n",
    "    #head_channels_image = np.array(head_channels_image.resize((w, h)))\n",
    "    composite_img[h:2*h, w:2*w, 0] = heatmap_image\n",
    "    composite_img[h:2*h, w:2*w, 1] = heatmap_image\n",
    "    composite_img[h:2*h, w:2*w, 2] = heatmap_image\n",
    "    # Save composite_img\n",
    "    composite_file_name = f\"composite_{os.path.basename(image_path)}\"\n",
    "    cv2.imwrite(os.path.join(demo_dir, composite_file_name), cv2.cvtColor(composite_img, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "\n",
    "def plot_image(image, gaze_heatmap_pred, depth, face, head_channel):\n",
    "\n",
    "    plt.figure(figsize=(16, 4))\n",
    "\n",
    "    # Original Image with Heatmap\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "    original_image = image.cpu() * std + mean\n",
    "    original_image = original_image[0][[0, 1, 2], :, :]\n",
    "    original_image = transforms.ToPILImage()(original_image).convert(\"RGB\")\n",
    "    original_image = np.array(original_image.resize((224, 224)))\n",
    "    heatmap_image_resized = np.array(gaze_heatmap_pred.resize((224, 224)))\n",
    "    heatmap_image_resized = np.stack([heatmap_image_resized, heatmap_image_resized, 0 * heatmap_image_resized], axis=2)\n",
    "    masked_image = cv2.addWeighted(original_image, 1, heatmap_image_resized, 2, 0)\n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.imshow(masked_image)\n",
    "    plt.title(\"Image with Heatmap\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Depth Image\n",
    "    depths_image = transforms.ToPILImage()(depth.cpu()).convert(\"RGB\")\n",
    "    depths_image = np.array(depths_image.resize((224, 224)))\n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.imshow(depths_image)\n",
    "    plt.title(\"Depth\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Faces Image\n",
    "    faces_image = face.cpu() * std + mean\n",
    "    faces_image = faces_image[0][[0, 1, 2], :, :]\n",
    "    faces_image = transforms.ToPILImage()(faces_image).convert(\"RGB\")\n",
    "    faces_image = np.array(faces_image.resize((224, 224)))\n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.imshow(faces_image)\n",
    "    plt.title(\"Faces\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Head Channels Image\n",
    "    head_channels_image = transforms.ToPILImage()(head_channel.cpu()).convert(\"L\")\n",
    "    head_channels_image = np.array(head_channels_image.resize((224, 224)))\n",
    "    plt.subplot(1, 4, 4)\n",
    "    plt.imshow(head_channels_image, cmap='gray')\n",
    "    plt.title(\"Head Channels\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def predict_image_without_annotation(dataset_dir, image_list,startbox, device=\"cuda\",plot=True, heatmap=True, Demo=True,coefficient=2, depth_mode=\"midas\"):\n",
    "    #generate annotation under dataset_dir\n",
    "    print(\"======Generate Head Box======\")\n",
    "    generate_facemask(image_list,startbox,dataset_dir,coefficient=coefficient) #Genearte a annotation file\n",
    "    predict_image_with_annotation(dataset_dir, image_list, device=\"cuda\",plot=plot, heatmap=heatmap, Demo=Demo,depth_mode=depth_mode)\n",
    "\n",
    "def predict_image_with_annotation(dataset_dir, image_list, device=\"cuda\",plot=True, heatmap=True, Demo=True, depth_mode=\"midas\"):\n",
    "\n",
    "# dataset_dir: The Dir should in style of gazefollow, that contains head_information.txt and file named image_original contains all images\n",
    "# checkpoint_dir: The dir of model weight.\n",
    "# image_list: A list of image name, should be same with first column in file test_annotations_release.txt. If image_name=“ALL”, it process all image detected.\n",
    "# ====== Output =======\n",
    "# if plot, only plot the result 2 image per row;\n",
    "# if heatmap, save heatmap prediction to heatmap_predict under dataset_dir;\n",
    "# if demo, save square diagram into heatmap_predict under dataset_dir.\n",
    "\n",
    "\n",
    "# Get config\n",
    "    print(\"======Loading Config======\")\n",
    "    config= get_config()\n",
    "    config= update_config(config,dataset_dir, device)\n",
    "    device = torch.device(config.device)\n",
    "    print(f\"Running on {device}\")\n",
    "\n",
    "# Make Datasets\n",
    "    print(\"======Loading dataset======\")\n",
    "    target_test_loader = test_loader(config,image_list)\n",
    "\n",
    "# Load model\n",
    "    print(\"======Loading model======\")\n",
    "    model = get_model(config, device=device)\n",
    "    pretrained_dict = torch.load(config.eval_weights, map_location=device)\n",
    "    pretrained_dict = pretrained_dict.get(\"model_state_dict\") or pretrained_dict.get(\"model\")\n",
    "    model = load_pretrained(model, pretrained_dict)\n",
    "\n",
    "# Process Depth File\n",
    "    print(\"======Process Depth======\")\n",
    "    input_path=os.path.join(dataset_dir,\"image_original\")\n",
    "    output_path=os.path.join(dataset_dir,\"depth_intermediate\")\n",
    "    model_weights=\"weights\\\\dpt_large-midas-2f21e586.pt\"\n",
    "    \n",
    "    if depth_mode==\"midas\":\n",
    "        extract_depth(input_path, output_path, model_weights, \"dpt_large\", \"no-optimize\", image_list)\n",
    "    else:\n",
    "        extract_depth_dino(input_path, output_path, model_weights, \"dpt_large\", \"no-optimize\", image_list)\n",
    "        \n",
    "# Prediction (dataloader, model, config --> saved image prediction)\n",
    "    print(\"======Prediction======\")\n",
    "    model.eval()\n",
    "    gaze_inside_all = []\n",
    "    gaze_inside_pred_all = []\n",
    "    # # Prediction (dataloader, model, config --> batched prediction array)\n",
    "    with torch.no_grad():\n",
    "        for batch, data in tqdm(enumerate(target_test_loader)):\n",
    "            (\n",
    "                images,\n",
    "                depths,\n",
    "                faces,\n",
    "                head_channels,\n",
    "                img_size,\n",
    "                path,\n",
    "            ) = data\n",
    "            images = images.to(device, non_blocking=True, memory_format=get_memory_format(config))\n",
    "            depths = depths.to(device, non_blocking=True, memory_format=get_memory_format(config))\n",
    "            faces = faces.to(device, non_blocking=True, memory_format=get_memory_format(config))\n",
    "            head = head_channels.to(device, non_blocking=True, memory_format=get_memory_format(config))\n",
    "            gaze_heatmap_pred, gaze_inside_pred, _, _, _ = model(images, depths, head, faces)\n",
    "            gaze_inside_pred_all.extend(gaze_inside_pred.squeeze(1).cpu().tolist())\n",
    "            gaze_heatmap_pred = gaze_heatmap_pred.squeeze(1).cpu()\n",
    "\n",
    "    # # Prediction (batched prediction array --> save)\n",
    "            # Define the directory where you want to save the predicted images\n",
    "            heatmap_dir = os.path.join(config.dataset_dir,\"predict_heatmap\")\n",
    "            demo_dir = os.path.join(config.dataset_dir,\"predict_demo\")\n",
    "            os.makedirs(heatmap_dir, exist_ok=True)  # Ensure the directory exists\n",
    "            os.makedirs(demo_dir, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "            # heatmap array, Address --> Saving the gaze_heatmap_pred image\n",
    "            for i, image_path in enumerate(path):\n",
    "\n",
    "                heatmap_image = gaze_heatmap_pred[i]\n",
    "                heatmap_image = transforms.ToPILImage()(heatmap_image)\n",
    "\n",
    "                if heatmap:\n",
    "                    heatmap_image.save(os.path.join(heatmap_dir, os.path.basename(image_path)))\n",
    "\n",
    "                if plot:\n",
    "                    plot_image(images[i], heatmap_image, depths[i], faces[i], head_channels[i])\n",
    "\n",
    "                if Demo:\n",
    "                    composition_image(os.path.join(dataset_dir,image_path), images[i], heatmap_image, depths[i], faces[i], head_channels[i],demo_dir)\n",
    "    print(\"ALl finished\")\n",
    "\n",
    "\n",
    "def predict_video(vid_dir, startbox, sampling_fps, device=\"cuda\",annotated=False, depth_mode=\"midas\"):\n",
    "    \"\"\"\n",
    "    Predict on video frames.\n",
    "\n",
    "    Parameters:\n",
    "    - vid_dir: The path to the video.\n",
    "    - save_dir: Directory to save extracted images.\n",
    "    - ref_img: The path to the reference image.\n",
    "    - sampling_fps: FPS to sample frames from the video.\n",
    "    - device: Device to use for prediction (default is \"cuda\").\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"======Load Video======\")\n",
    "    # Use OpenCV to get video properties\n",
    "    cap = cv2.VideoCapture(vid_dir)\n",
    "    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    video_fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    cap.release()\n",
    "\n",
    "    print(f\"Length: {video_length} frames, Frequency: {video_fps} FPS\")\n",
    "\n",
    "    if sampling_fps > video_fps:\n",
    "        raise ValueError(\"The sampling FPS cannot be greater than the video's FPS.\")\n",
    "\n",
    "    # Calculate frame extraction frequency based on desired sampling FPS\n",
    "    freq = video_fps // sampling_fps\n",
    "\n",
    "    print(\"======Extract Image from Video======\")\n",
    "\n",
    "    # Create a directory to save extracted images\n",
    "    basename_without_ext = os.path.splitext(os.path.basename(vid_dir))[0]\n",
    "    save_dir=os.path.join(os.path.dirname(vid_dir),basename_without_ext)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    img_dir = os.path.join(save_dir, \"image_original\")\n",
    "    if not os.path.exists(img_dir):\n",
    "        os.makedirs(img_dir, exist_ok=True)\n",
    "\n",
    "    # Extract frames from the video\n",
    "    cap = cv2.VideoCapture(vid_dir)\n",
    "    frame_number = 0\n",
    "\n",
    "    for _ in tqdm(range(video_length), desc=\"Extracting Frames\"):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_number % freq == 0:\n",
    "            frame_path = os.path.join(img_dir, f\"frame_{frame_number}.jpg\")\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "        frame_number += 1\n",
    "    cap.release()\n",
    "\n",
    "    # Call predict_image_without_annotation function\n",
    "    if annotated:\n",
    "        predict_image_with_annotation(save_dir, \"ALL\", plot=False,device=device, depth_mode=depth_mode)\n",
    "    else:\n",
    "        predict_image_without_annotation(save_dir, \"ALL\", startbox=startbox, plot=False,device=device,coefficient=2,depth_mode=depth_mode)\n",
    "\n",
    "    print(\"======Import Image to Video======\")\n",
    "    # Output video path\n",
    "    output_video_path = os.path.join(save_dir, f\"predict_demo/{basename_without_ext}_output.avi\")\n",
    "    # Get size info from the first image\n",
    "    first_image_path = os.path.join(save_dir, \"predict_demo/composite_frame_0.jpg\")\n",
    "    img_sample = cv2.imread(first_image_path)\n",
    "    height, width, _ = img_sample.shape\n",
    "\n",
    "    # Set up video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Can be changed based on codecs available on the system\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, sampling_fps, (width, height))\n",
    "\n",
    "    # Read all processed images and write them to the video\n",
    "    for i in tqdm(range(0, video_length, freq), desc=\"Importing Frames\"):\n",
    "        frame_path = os.path.join(save_dir, f\"predict_demo/composite_frame_{i}.jpg\")\n",
    "        img = cv2.imread(frame_path)\n",
    "        out.write(img)\n",
    "\n",
    "    out.release()\n",
    "    print(f\"Processed video saved to {output_video_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method1: With given teacher start position\n",
    "\n",
    "**Given**: \n",
    "\n",
    "a. Video path\n",
    "\n",
    "b. The tracking information needed for teacher.\n",
    "(eg: open the video ---> pause at the first frame ---> navigate roughtly bbox of teacher head. )\n",
    "\n",
    "**Wanted**:\n",
    "\n",
    "Predicted Gaze Image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T20:18:01.308101200Z",
     "start_time": "2023-10-10T20:17:34.662669800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Load Video======\n",
      "Length: 751 frames, Frequency: 25 FPS\n",
      "======Extract Image from Video======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Frames: 100%|████████████████████████████████████████████████████████████▉| 750/751 [00:07<00:00, 99.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Loading Config======\n",
      "Running on cuda\n",
      "======Loading dataset======\n",
      "======Loading model======\n",
      "Total params: 92183098\n",
      "Total trainable params: 92183098\n",
      "<All keys matched successfully>\n",
      "======Process Depth======\n",
      "initialize\n",
      "device: cuda\n",
      "start processing\n",
      "\n",
      " Working on folder C:\\Datasets\\Engagement\\VIDEO000\\00000\\image_original\n",
      "63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:30,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n",
      "======Prediction======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:29,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALl finished\n",
      "======Import Image to Video======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing Frames: 100%|████████████████████████████████████████████████████████████████| 63/63 [00:04<00:00, 15.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed video saved to C:\\Datasets\\Engagement\\VIDEO000\\00000\\predict_demo/00000_output.avi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "video_path=r\"C:\\Datasets\\Engagement\\VIDEO000\\00000.avi\"\n",
    "start_bbox_of_teacher_in_Video=[267,200,314,248]\n",
    "sampling_fps=2\n",
    "\n",
    "predict_video(video_path,startbox=start_bbox_of_teacher_in_Video,sampling_fps=sampling_fps,annotated=True,depth_mode=\"midas\")\n",
    "#predict_video(video_path,startbox=start_bbox_of_teacher_in_Video,sampling_fps=sampling_fps,annotated=True,depth_mode=\"dino\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-30T09:54:58.019714800Z",
     "start_time": "2023-09-30T09:54:58.017227700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Method2: With given teacher segmentation\n",
    "\n",
    "**Given**: \n",
    "\n",
    "a. Video path\n",
    "\n",
    "b. mask of teacher.\n",
    "\n",
    "**Wanted**:\n",
    "\n",
    "Predicted Gaze Image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-30T09:54:58.022274800Z",
     "start_time": "2023-09-30T09:54:58.019714800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "video_path=r\"D:\\Datasets\\engagement_follow\\Video\\00000.avi\"\n",
    "start_bbox_of_teacher_in_Video=[267,200,314,248]\n",
    "sampling_fps=2\n",
    "\n",
    "#predict_video_with_segmentation(video_path,startbox=start_bbox_of_teacher_in_Video,sampling_fps=sampling_fps,annotated=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T14:39:39.426908800Z",
     "start_time": "2023-10-06T14:39:39.409906400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
