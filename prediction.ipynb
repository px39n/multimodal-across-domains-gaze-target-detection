{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Library\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMP is not available\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from config import get_config,update_config\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    from apex import amp\n",
    "except ImportError:\n",
    "    print(\"AMP is not available\")\n",
    "from datasets.predict_loader import test_loader\n",
    "from models import get_model, load_pretrained\n",
    "from utils import get_memory_format\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T13:38:55.661073200Z",
     "start_time": "2023-10-07T13:38:53.500909500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 1 (517861323.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Cell \u001B[1;32mIn[3], line 4\u001B[1;36m\u001B[0m\n\u001B[1;33m    for vid in vid_dirs:\u001B[0m\n\u001B[1;37m    ^\u001B[0m\n\u001B[1;31mIndentationError\u001B[0m\u001B[1;31m:\u001B[0m expected an indented block after function definition on line 1\n"
     ]
    }
   ],
   "source": [
    "def predict_videos(vid_dirs, save_dir, device=\"cuda\",freq):\n",
    "# vid_dirs: A list, contains all videos address\n",
    "# ====== Output =======\n",
    "for vid in vid_dirs:\n",
    "    save_sub_dir=\"videoname without ext\"\n",
    "    predict_video(vid,save_sub_dir,device=device,freq=freq)\n",
    "\n",
    "\n",
    "def predict_video(vid_dir, save_dir, device=\"cuda\",freq):\n",
    "# vid_dirs: video address (absolute)\n",
    "# ====== Output =======\n",
    "get_videotype(vid_dir)\n",
    "print(detect video xxx, print meta information)\n",
    "\n",
    "create a  file called temp under save_dir\n",
    "extract video2Image(vid_dir, temp, freq)\n",
    "\n",
    "predict_image(temp, \"ALL\", plot=True, heatmap=True, Demo=True, device=\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T16:00:22.897038Z",
     "start_time": "2023-10-06T16:00:22.890392500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def predict_image(dataset_dir, image_list, device=\"cuda\",plot=True, heatmap=True, Demo=True):\n",
    "\n",
    "# dataset_dir: The Dir should in style of gazefollow, that contains head_information.txt and file named image_original contains all images\n",
    "# checkpoint_dir: The dir of model weight.\n",
    "# image_list: A list of image name, should be same with first column in file test_annotations_release.txt. If image_name=“ALL”, it process all image detected.\n",
    "# ====== Output =======\n",
    "# if plot, only plot the result 2 image per row;\n",
    "# if heatmap, save heatmap prediction to heatmap_predict under dataset_dir;\n",
    "# if demo, save square diagram into heatmap_predict under dataset_dir.\n",
    "\n",
    "\n",
    "# Get config\n",
    "    config= get_config()\n",
    "    config= update_config(config,dataset_dir, device)\n",
    "    device = torch.device(config.device)\n",
    "    print(f\"Running on {device}\")\n",
    "\n",
    "# Make Datasets\n",
    "    print(\"Loading dataset\")\n",
    "    target_test_loader = test_loader(config,image_list)\n",
    "\n",
    "# Load model\n",
    "    print(\"Loading model\")\n",
    "    model = get_model(config, device=device)\n",
    "    print(model)\n",
    "    pretrained_dict = torch.load(config.eval_weights, map_location=device)\n",
    "    pretrained_dict = pretrained_dict.get(\"model_state_dict\") or pretrained_dict.get(\"model\")\n",
    "    model = load_pretrained(model, pretrained_dict)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T13:38:55.670447700Z",
     "start_time": "2023-10-07T13:38:55.663073500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#predict_image(r\"D:\\Datasets\\engagement_follow\",[\"image_original/00000004.jpg\"])\n",
    "\n",
    "#predict_image(r\"D:\\Datasets\\engagement_follow\",[\"image_original/00000004.jpg\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "output_size = config.output_size\n",
    "print_every = config.print_every\n",
    "\n",
    "gaze_inside_all = []\n",
    "gaze_inside_pred_all = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch, data in enumerate(target_test_loader):\n",
    "        (\n",
    "            images,\n",
    "            depths,\n",
    "            faces,\n",
    "            head_channels,\n",
    "            _,\n",
    "            eye_coords,\n",
    "            gaze_coords,\n",
    "            gaze_inside,\n",
    "            img_size,\n",
    "            path,\n",
    "        ) = data\n",
    "\n",
    "        images = images.to(device, non_blocking=True, memory_format=get_memory_format(config))\n",
    "        depths = depths.to(device, non_blocking=True, memory_format=get_memory_format(config))\n",
    "        faces = faces.to(device, non_blocking=True, memory_format=get_memory_format(config))\n",
    "        head = head_channels.to(device, non_blocking=True, memory_format=get_memory_format(config))\n",
    "        gaze_inside = gaze_inside.to(device, non_blocking=True, memory_format=get_memory_format(config))\n",
    "\n",
    "        gaze_heatmap_pred, gaze_inside_pred, _, _, _ = model(images, depths, head, faces)\n",
    "\n",
    "        gaze_inside_all.extend(gaze_inside.cpu().tolist())\n",
    "        gaze_inside_pred_all.extend(gaze_inside_pred.squeeze(1).cpu().tolist())\n",
    "        gaze_heatmap_pred = gaze_heatmap_pred.squeeze(1).cpu()\n",
    "\n",
    "        import os\n",
    "        import cv2\n",
    "        import torchvision.transforms as transforms\n",
    "        for i, image_path in enumerate(path):\n",
    "                # Extracting file name from the path\n",
    "                file_name = os.path.basename(image_path)  # Gets '123.jpg' from 'test/123.jpg'\n",
    "\n",
    "                # Define the directory where you want to save the predicted images\n",
    "                save_dir = config.save_dir\n",
    "                os.makedirs(save_dir, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "                # Check if the file already exists. If so, modify the file name\n",
    "                base_name, ext = os.path.splitext(file_name)  # Gets '123' and '.jpg' from '123.jpg'\n",
    "                count = 0\n",
    "                while os.path.exists(os.path.join(save_dir, file_name)):\n",
    "                    count += 1\n",
    "                    file_name = f\"{base_name}_{count}{ext}\"  # Modifies '123.jpg' to '123_1.jpg' etc.\n",
    "\n",
    "                # Saving the gaze_heatmap_pred image\n",
    "                heatmap_image = gaze_heatmap_pred[i]  # Assuming gaze_heatmap_pred has per-image predictions in the first dimension\n",
    "\n",
    "                # Convert the heatmap tensor to an image and save it\n",
    "                heatmap_image = transforms.ToPILImage()(heatmap_image)\n",
    "                heatmap_image.save(os.path.join(save_dir, file_name))\n",
    "\n",
    "                # Create an empty array to store the final composite image\n",
    "                composite_img = np.zeros((448, 448, 3), dtype=np.uint8)\n",
    "\n",
    "\n",
    "                mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
    "                std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "\n",
    "                # left up\n",
    "                original_image = images[i].cpu() * std + mean\n",
    "                original_image = original_image[0][[0, 1, 2], :, :]\n",
    "                original_image = transforms.ToPILImage()(original_image).convert(\"RGB\")\n",
    "                original_image = np.array(original_image.resize((224, 224)))\n",
    "                heatmap_image_resized = heatmap_image.resize((224, 224))\n",
    "                heatmap_image_resized = np.array(heatmap_image_resized)\n",
    "                heatmap_image_resized = np.stack([heatmap_image_resized * 1, heatmap_image_resized* 1, heatmap_image_resized * 0], axis=2)\n",
    "                masked_image = cv2.addWeighted(original_image, 1, heatmap_image_resized, 2, 0)\n",
    "                composite_img[0:224, 0:224, :] = masked_image\n",
    "\n",
    "\n",
    "\n",
    "                # LEFT DOWN: Depths image\n",
    "                depths_image = transforms.ToPILImage()(depths[i].cpu()).convert(\"RGB\")\n",
    "                depths_image = np.array(depths_image.resize((224, 224)))\n",
    "                composite_img[224:448, 0:224, :] = depths_image\n",
    "\n",
    "                # RIGHT UP: Faces image\n",
    "                faces_image = faces[i].cpu()* std + mean\n",
    "                faces_image = faces_image[0][[0, 1, 2], :, :]\n",
    "                faces_image = transforms.ToPILImage()(faces_image).convert(\"RGB\")\n",
    "                faces_image = np.array(faces_image.resize((224, 224)))\n",
    "                composite_img[0:224, 224:448, :] = faces_image\n",
    "\n",
    "                # RIGHT DOWN: Head_channels image (grayscale)\n",
    "                head_channels_image = transforms.ToPILImage()(head_channels[i].cpu()).convert(\"L\")\n",
    "                head_channels_image = np.array(head_channels_image.resize((224, 224)))\n",
    "                composite_img[224:448, 224:448, 0] = head_channels_image\n",
    "                composite_img[224:448, 224:448, 1] = head_channels_image\n",
    "                composite_img[224:448, 224:448, 2] = head_channels_image\n",
    "                # Save composite_img\n",
    "                composite_file_name = f\"composite_{file_name}\"\n",
    "                cv2.imwrite(os.path.join(save_dir, composite_file_name), cv2.cvtColor(composite_img, cv2.COLOR_RGB2BGR))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T09:54:58.013845500Z",
     "start_time": "2023-09-30T09:54:51.617015500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=L size=64x64 at 0x24454732810>\n"
     ]
    }
   ],
   "source": [
    "print(heatmap_image)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T09:54:58.018207100Z",
     "start_time": "2023-09-30T09:54:58.014845300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T09:54:58.019714800Z",
     "start_time": "2023-09-30T09:54:58.017227700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T09:54:58.022274800Z",
     "start_time": "2023-09-30T09:54:58.019714800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T14:39:39.426908800Z",
     "start_time": "2023-10-06T14:39:39.409906400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
